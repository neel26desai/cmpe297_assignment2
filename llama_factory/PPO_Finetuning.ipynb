{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN/q/uSXU4G4vkZHwy5l2Mt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neel26desai/cmpe297_assignment2/blob/main/llama_factory/PPO_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_LggpctJ4_J",
        "outputId": "6c89847b-e936-4319-c930-dec83ebee55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir /content/drive/MyDrive/cmpe_297_assignmet2\n",
        "%cd /content/drive/MyDrive/cmpe_297_assignmet2\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
        "!pip uninstall -y jax\n",
        "!pip install -e .[torch,bitsandbytes,liger-kernel]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC3G8q-kKOsM",
        "outputId": "e009bbb1-3c72-43cf-f4cc-94bf611b0d82"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/cmpe_297_assignmet2’: File exists\n",
            "/content/drive/MyDrive/cmpe_297_assignmet2\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 315, done.\u001b[K\n",
            "remote: Counting objects: 100% (315/315), done.\u001b[K\n",
            "remote: Compressing objects: 100% (245/245), done.\u001b[K\n",
            "remote: Total 315 (delta 80), reused 159 (delta 57), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (315/315), 8.94 MiB | 15.86 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n",
            "Updating files: 100% (257/257), done.\n",
            "/content/drive/MyDrive/cmpe_297_assignmet2/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "Collecting torch==2.3.1\n",
            "  Using cached torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.18.1\n",
            "  Using cached torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio==2.3.1\n",
            "  Using cached torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.1 (from torch==2.3.1)\n",
            "  Using cached triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (9.4.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
            "Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.22.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.22.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.22.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.0+cu121\n",
            "    Uninstalling torch-2.4.0+cu121:\n",
            "      Successfully uninstalled torch-2.4.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.19.0+cu121\n",
            "    Uninstalling torchvision-0.19.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.19.0+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.4.0+cu121\n",
            "    Uninstalling torchaudio-2.4.0+cu121:\n",
            "      Successfully uninstalled torchaudio-2.4.0+cu121\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 torch-2.3.1 torchaudio-2.3.1 torchvision-0.18.1 triton-2.3.1\n",
            "Found existing installation: jax 0.4.26\n",
            "Uninstalling jax-0.4.26:\n",
            "  Successfully uninstalled jax-0.4.26\n",
            "Obtaining file:///content/drive/MyDrive/cmpe_297_assignmet2/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<=4.45.0,>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (4.44.2)\n",
            "Collecting datasets<=2.21.0,>=2.16.0 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: accelerate<=0.34.2,>=0.30.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.34.2)\n",
            "Collecting peft<=0.12.0,>=0.11.1 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting gradio>=4.0.0 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading gradio-4.44.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (1.13.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (0.1.99)\n",
            "Collecting tiktoken (from llamafactory==0.9.1.dev0)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (3.20.3)\n",
            "Collecting uvicorn (from llamafactory==0.9.1.dev0)\n",
            "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.9.1)\n",
            "Collecting fastapi (from llamafactory==0.9.1.dev0)\n",
            "  Downloading fastapi-0.114.2-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.9.1.dev0)\n",
            "  Downloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (3.7.1)\n",
            "Collecting fire (from llamafactory==0.9.1.dev0)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (1.26.4)\n",
            "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.9.1.dev0)\n",
            "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting liger-kernel (from llamafactory==0.9.1.dev0)\n",
            "  Downloading liger_kernel-0.3.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.1.dev0) (2.3.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (0.24.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=0.34.2,>=0.30.1->llamafactory==0.9.1.dev0) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.16.0)\n",
            "Collecting pyarrow>=15.0.0 (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.66.5)\n",
            "Collecting xxhash (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.10.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.7.1)\n",
            "Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (9.4.0)\n",
            "Collecting pydub (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading ruff-0.6.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.0.7)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi->llamafactory==0.9.1.dev0)\n",
            "  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.1.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.1.dev0) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.1.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.1.dev0) (2.23.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.9.1.dev0) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.9.1.dev0) (12.6.68)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.45.0,>=4.41.2->llamafactory==0.9.1.dev0) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<=4.45.0,>=4.41.2->llamafactory==0.9.1.dev0) (0.19.1)\n",
            "Collecting tyro>=0.5.11 (from trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0)\n",
            "  Downloading tyro-0.8.10-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.1.dev0) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn->llamafactory==0.9.1.dev0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.1.dev0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.1.dev0) (2.4.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.2.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.9.1.dev0) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.9.1.dev0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=2.21.0,>=2.16.0->llamafactory==0.9.1.dev0) (3.3.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (13.8.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->llamafactory==0.9.1.dev0)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.9.1.dev0) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.9.1.dev0) (0.1.2)\n",
            "Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.44.0-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading fastapi-0.114.2-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading liger_kernel-0.3.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.38.5-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.10-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.1.dev0-0.editable-py3-none-any.whl size=22382 sha256=c539186e0253deb52c4751d0f1ad6623a972d91128b4de9fea65dbdcbe27de3e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-za_2j8af/wheels/43/d5/68/7834394a03910e887e64b8df81e302f4ae7419c62838bb8b34\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117030 sha256=f36ab913be00551566df7c8a831df8d27aa71bda5589171b92a758f0f9a94728\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built llamafactory fire\n",
            "Installing collected packages: pydub, xxhash, websockets, tomlkit, shtab, semantic-version, ruff, python-multipart, pyarrow, orjson, h11, fire, ffmpy, dill, aiofiles, uvicorn, tiktoken, starlette, multiprocess, httpcore, tyro, sse-starlette, httpx, fastapi, gradio-client, bitsandbytes, peft, liger-kernel, gradio, datasets, trl, llamafactory\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.2\n",
            "    Uninstalling tomlkit-0.13.2:\n",
            "      Successfully uninstalled tomlkit-0.13.2\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 bitsandbytes-0.43.3 datasets-2.21.0 dill-0.3.8 fastapi-0.114.2 ffmpy-0.4.0 fire-0.6.0 gradio-4.44.0 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 liger-kernel-0.3.0 llamafactory-0.9.1.dev0 multiprocess-0.70.16 orjson-3.10.7 peft-0.12.0 pyarrow-17.0.0 pydub-0.25.1 python-multipart-0.0.9 ruff-0.6.5 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.1.3 starlette-0.38.5 tiktoken-0.7.0 tomlkit-0.12.0 trl-0.9.6 tyro-0.8.10 uvicorn-0.30.6 websockets-12.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install liger-kernel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o3mPtKcKjKa",
        "outputId": "78e45e0e-532a-49f4-e3b4-548f2ea8d9fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-5d4uh8gg\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-5d4uh8gg\n",
            "  Resolved https://github.com/huggingface/transformers to commit 8bd2b1e8c23234cd607ca8d63f53c1edfea27462\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0) (2024.8.30)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9732511 sha256=baaea8cf7060ae0cf00bed946a0daa31013d57459615f6d98440001ae415ebbb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2pliz15d/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed transformers-4.45.0.dev0\n",
            "Requirement already satisfied: liger-kernel in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from liger-kernel) (2.3.1)\n",
            "Requirement already satisfied: triton>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from liger-kernel) (2.3.1)\n",
            "Requirement already satisfied: transformers>=4.42.0 in /usr/local/lib/python3.10/dist-packages (from liger-kernel) (4.45.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.2->liger-kernel) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.2->liger-kernel) (12.6.68)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.0->liger-kernel) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.0->liger-kernel) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.0->liger-kernel) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.0->liger-kernel) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.0->liger-kernel) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.0->liger-kernel) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.0->liger-kernel) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.0->liger-kernel) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.42.0->liger-kernel) (4.66.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.2->liger-kernel) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.42.0->liger-kernel) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.42.0->liger-kernel) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.42.0->liger-kernel) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.42.0->liger-kernel) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.2->liger-kernel) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ],
      "metadata": {
        "id": "Du42Os7TK8dM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "%cd /content/drive/MyDrive/cmpe_297_assignmet2/LLaMA-Factory\n",
        "\n",
        "NAME = \"Llama-3\"\n",
        "AUTHOR = \"LLaMA Factory\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJmdBPOEK_eg",
        "outputId": "93730520-c0e9-4bf7-a789-70438444c08f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/cmpe_297_assignmet2/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN2 = userdata.get('HF_TOKEN2')"
      ],
      "metadata": {
        "id": "oXjDFgwsLKZt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $HF_TOKEN2 --add-to-git-credential"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdfjYZaCLMOj",
        "outputId": "40fd7630-d811-43ad-cd04-1a5b79fd116a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = {\n",
        "    ### model\n",
        "    \"model_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    \"reward_model\": \"saves/llama3-8b/lora/reward\",\n",
        "\n",
        "    ### method\n",
        "    \"stage\": \"ppo\",\n",
        "    \"do_train\": True,\n",
        "    \"finetuning_type\": \"lora\",\n",
        "    \"lora_target\": \"all\",\n",
        "\n",
        "    ### dataset\n",
        "    \"dataset\": \"identity,alpaca_en_demo\",\n",
        "    \"template\": \"llama3\",\n",
        "    \"cutoff_len\": 1024,\n",
        "    \"max_samples\": 1000,\n",
        "    \"overwrite_cache\": True,\n",
        "    \"preprocessing_num_workers\": 16,\n",
        "\n",
        "    ### output\n",
        "    \"output_dir\": \"/conten/drive/MyDrive/saves/llam3/ppo\",\n",
        "    \"logging_steps\": 10,\n",
        "    \"save_steps\": 500,\n",
        "    \"plot_loss\": True,\n",
        "    \"overwrite_output_dir\": True,\n",
        "\n",
        "    ### train\n",
        "    \"per_device_train_batch_size\": 1,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"learning_rate\": 1.0e-5,\n",
        "    \"num_train_epochs\": 3.0,\n",
        "    \"lr_scheduler_type\": \"cosine\",\n",
        "    \"warmup_ratio\": 0.1,\n",
        "    \"bf16\": True,\n",
        "    \"ddp_timeout\": 180000000,\n",
        "\n",
        "    ### generate\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"top_k\": 0,\n",
        "    \"top_p\": 0.9\n",
        "}\n",
        "\n",
        "\n",
        "json.dump(args, open(\"train_llama3_ppo.json\", \"w\", encoding=\"utf-8\"), indent=2)\n"
      ],
      "metadata": {
        "id": "e_BeyxgDLONd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZltgHBCMR3Z",
        "outputId": "346c3cd1-ff76-4722-ce5a-419f666ee64d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/LLaMA-Factory\n",
            "2024-09-15 19:51:12.765053: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-09-15 19:51:12.783325: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-15 19:51:12.804726: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-15 19:51:12.811244: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-15 19:51:12.826911: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-15 19:51:14.053369: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "09/15/2024 19:51:21 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
            "config.json: 100% 1.20k/1.20k [00:00<00:00, 6.93MB/s]\n",
            "[INFO|configuration_utils.py:672] 2024-09-15 19:51:22,462 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/config.json\n",
            "[WARNING|modeling_rope_utils.py:379] 2024-09-15 19:51:22,463 >> Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
            "[INFO|configuration_utils.py:739] 2024-09-15 19:51:22,465 >> Model config Qwen2VLConfig {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2_vl\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.45.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"in_chans\": 3,\n",
            "    \"model_type\": \"qwen2_vl\",\n",
            "    \"spatial_patch_size\": 14\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 4.19k/4.19k [00:00<00:00, 20.6MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 4.25MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 7.25MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 8.01MB/s]\n",
            "[INFO|tokenization_utils_base.py:2215] 2024-09-15 19:51:26,774 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2215] 2024-09-15 19:51:26,774 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2215] 2024-09-15 19:51:26,774 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2215] 2024-09-15 19:51:26,774 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2215] 2024-09-15 19:51:26,774 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2215] 2024-09-15 19:51:26,774 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2479] 2024-09-15 19:51:27,069 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "preprocessor_config.json: 100% 347/347 [00:00<00:00, 1.92MB/s]\n",
            "[INFO|image_processing_base.py:375] 2024-09-15 19:51:28,093 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/preprocessor_config.json\n",
            "[INFO|image_processing_base.py:375] 2024-09-15 19:51:28,371 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/preprocessor_config.json\n",
            "[INFO|image_processing_base.py:429] 2024-09-15 19:51:28,372 >> Image processor Qwen2VLImageProcessor {\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"max_pixels\": 12845056,\n",
            "  \"merge_size\": 2,\n",
            "  \"min_pixels\": 3136,\n",
            "  \"patch_size\": 14,\n",
            "  \"processor_class\": \"Qwen2VLProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"max_pixels\": 12845056,\n",
            "    \"min_pixels\": 3136\n",
            "  },\n",
            "  \"temporal_patch_size\": 2\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2215] 2024-09-15 19:51:28,609 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2215] 2024-09-15 19:51:28,609 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2215] 2024-09-15 19:51:28,609 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2215] 2024-09-15 19:51:28,609 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2215] 2024-09-15 19:51:28,609 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2215] 2024-09-15 19:51:28,609 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2479] 2024-09-15 19:51:28,876 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "chat_template.json: 100% 1.05k/1.05k [00:00<00:00, 7.32MB/s]\n",
            "[INFO|processing_utils.py:724] 2024-09-15 19:51:30,023 >> Processor Qwen2VLProcessor:\n",
            "- image_processor: Qwen2VLImageProcessor {\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"Qwen2VLImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"max_pixels\": 12845056,\n",
            "  \"merge_size\": 2,\n",
            "  \"min_pixels\": 3136,\n",
            "  \"patch_size\": 14,\n",
            "  \"processor_class\": \"Qwen2VLProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"max_pixels\": 12845056,\n",
            "    \"min_pixels\": 3136\n",
            "  },\n",
            "  \"temporal_patch_size\": 2\n",
            "}\n",
            "\n",
            "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2-VL-7B-Instruct', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            "\n",
            "{\n",
            "  \"processor_class\": \"Qwen2VLProcessor\"\n",
            "}\n",
            "\n",
            "09/15/2024 19:51:30 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
            "09/15/2024 19:51:30 - INFO - llamafactory.data.loader - Loading dataset llamafactory/RLHF-V...\n",
            "Downloading readme: 100% 751/751 [00:00<00:00, 3.04kB/s]\n",
            "Downloading data: 100% 365M/365M [00:01<00:00, 293MB/s]\n",
            "Generating train split: 5733 examples [00:01, 5694.48 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 1000/1000 [00:10<00:00, 93.65 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 1000/1000 [00:07<00:00, 131.87 examples/s]\n",
            "training example:\n",
            "chosen_input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 3838, 525, 279, 1376, 4419, 498, 22986, 304, 279, 2168, 30, 151645, 198, 151644, 77091, 198, 32, 3908, 883, 11259, 389, 6430, 12233, 264, 4158, 15478, 323, 3691, 24549, 13, 151645]\n",
            "chosen_inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>What are the key features you observe in the image?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "A young man standing on stage wearing a white shirt and black pants.<|im_end|>\n",
            "chosen_label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 32, 3908, 883, 11259, 389, 6430, 12233, 264, 4158, 15478, 323, 3691, 24549, 13, 151645]\n",
            "chosen_labels:\n",
            "A young man standing on stage wearing a white shirt and black pants.<|im_end|>\n",
            "rejected_input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 3838, 525, 279, 1376, 4419, 498, 22986, 304, 279, 2168, 30, 151645, 198, 151644, 77091, 198, 32, 3908, 883, 11259, 389, 6430, 12233, 4158, 24549, 323, 15294, 13, 151645]\n",
            "rejected_inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>What are the key features you observe in the image?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "A young man standing on stage wearing white pants and shoes.<|im_end|>\n",
            "rejected_label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 32, 3908, 883, 11259, 389, 6430, 12233, 4158, 24549, 323, 15294, 13, 151645]\n",
            "rejected_labels:\n",
            "A young man standing on stage wearing white pants and shoes.<|im_end|>\n",
            "[INFO|configuration_utils.py:672] 2024-09-15 19:51:56,822 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/config.json\n",
            "[WARNING|modeling_rope_utils.py:379] 2024-09-15 19:51:56,822 >> Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
            "[INFO|configuration_utils.py:739] 2024-09-15 19:51:56,824 >> Model config Qwen2VLConfig {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2_vl\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.45.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"in_chans\": 3,\n",
            "    \"model_type\": \"qwen2_vl\",\n",
            "    \"spatial_patch_size\": 14\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "model.safetensors.index.json: 100% 56.5k/56.5k [00:00<00:00, 146MB/s]\n",
            "[INFO|modeling_utils.py:3694] 2024-09-15 19:51:57,796 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/5 [00:00<?, ?it/s]\n",
            "model-00001-of-00005.safetensors:   0% 0.00/3.90G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   1% 21.0M/3.90G [00:00<00:19, 200MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   1% 52.4M/3.90G [00:00<00:15, 250MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   3% 105M/3.90G [00:00<00:10, 358MB/s] \u001b[A\n",
            "model-00001-of-00005.safetensors:   4% 157M/3.90G [00:00<00:09, 415MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   5% 210M/3.90G [00:00<00:08, 448MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   7% 262M/3.90G [00:00<00:07, 471MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:   8% 315M/3.90G [00:00<00:07, 487MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  10% 377M/3.90G [00:00<00:07, 500MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  11% 440M/3.90G [00:00<00:06, 510MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  13% 493M/3.90G [00:01<00:06, 505MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  14% 556M/3.90G [00:01<00:06, 512MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  16% 608M/3.90G [00:01<00:06, 512MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  17% 661M/3.90G [00:01<00:06, 511MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  18% 713M/3.90G [00:01<00:06, 508MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  20% 765M/3.90G [00:01<00:06, 509MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  21% 818M/3.90G [00:01<00:06, 508MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  22% 870M/3.90G [00:01<00:05, 508MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  24% 923M/3.90G [00:01<00:05, 503MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  25% 975M/3.90G [00:02<00:05, 503MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  26% 1.03G/3.90G [00:02<00:05, 502MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  28% 1.08G/3.90G [00:02<00:05, 502MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  29% 1.13G/3.90G [00:02<00:05, 499MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  30% 1.18G/3.90G [00:02<00:05, 499MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  32% 1.24G/3.90G [00:02<00:05, 505MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  33% 1.29G/3.90G [00:02<00:05, 505MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  34% 1.34G/3.90G [00:02<00:05, 502MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  36% 1.39G/3.90G [00:02<00:04, 508MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  37% 1.45G/3.90G [00:02<00:04, 506MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  38% 1.50G/3.90G [00:03<00:04, 508MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  40% 1.55G/3.90G [00:03<00:04, 505MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  41% 1.60G/3.90G [00:03<00:04, 502MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  42% 1.66G/3.90G [00:03<00:04, 499MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  44% 1.71G/3.90G [00:03<00:04, 498MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  45% 1.76G/3.90G [00:03<00:04, 498MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  47% 1.81G/3.90G [00:03<00:04, 495MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  48% 1.87G/3.90G [00:03<00:04, 499MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  49% 1.92G/3.90G [00:03<00:03, 504MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  51% 1.97G/3.90G [00:04<00:03, 498MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  52% 2.02G/3.90G [00:04<00:03, 495MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  53% 2.08G/3.90G [00:04<00:03, 493MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  55% 2.13G/3.90G [00:04<00:04, 422MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  56% 2.18G/3.90G [00:04<00:04, 359MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  57% 2.22G/3.90G [00:04<00:05, 325MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  58% 2.26G/3.90G [00:04<00:05, 294MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  59% 2.30G/3.90G [00:05<00:05, 268MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  60% 2.33G/3.90G [00:05<00:06, 233MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  61% 2.36G/3.90G [00:05<00:07, 202MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  61% 2.39G/3.90G [00:05<00:08, 177MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  62% 2.41G/3.90G [00:05<00:09, 164MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  62% 2.43G/3.90G [00:06<00:09, 158MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  63% 2.45G/3.90G [00:06<00:09, 157MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  63% 2.47G/3.90G [00:06<00:08, 160MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  64% 2.50G/3.90G [00:06<00:08, 159MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  65% 2.52G/3.90G [00:06<00:08, 161MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  65% 2.54G/3.90G [00:06<00:08, 161MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  66% 2.56G/3.90G [00:06<00:08, 159MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  66% 2.58G/3.90G [00:06<00:08, 159MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  67% 2.60G/3.90G [00:07<00:08, 157MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  67% 2.62G/3.90G [00:07<00:07, 165MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  69% 2.67G/3.90G [00:07<00:04, 250MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  70% 2.73G/3.90G [00:07<00:03, 319MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  71% 2.78G/3.90G [00:07<00:03, 370MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  73% 2.83G/3.90G [00:07<00:02, 408MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  74% 2.88G/3.90G [00:07<00:02, 437MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  75% 2.94G/3.90G [00:07<00:02, 452MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  77% 2.99G/3.90G [00:07<00:01, 457MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  78% 3.04G/3.90G [00:08<00:01, 450MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  79% 3.09G/3.90G [00:08<00:01, 457MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  81% 3.15G/3.90G [00:08<00:01, 468MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  82% 3.20G/3.90G [00:08<00:01, 476MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  83% 3.25G/3.90G [00:08<00:01, 476MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  85% 3.30G/3.90G [00:08<00:01, 466MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  86% 3.36G/3.90G [00:08<00:01, 463MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  87% 3.41G/3.90G [00:08<00:01, 477MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  89% 3.46G/3.90G [00:08<00:00, 477MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  90% 3.51G/3.90G [00:09<00:00, 466MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  91% 3.57G/3.90G [00:09<00:00, 468MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  93% 3.62G/3.90G [00:09<00:00, 469MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  94% 3.67G/3.90G [00:09<00:00, 478MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  95% 3.72G/3.90G [00:09<00:00, 486MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  97% 3.77G/3.90G [00:09<00:00, 357MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors:  98% 3.83G/3.90G [00:09<00:00, 390MB/s]\u001b[A\n",
            "model-00001-of-00005.safetensors: 100% 3.90G/3.90G [00:10<00:00, 390MB/s]\n",
            "Downloading shards:  20% 1/5 [00:10<00:41, 10.47s/it]\n",
            "model-00002-of-00005.safetensors:   0% 0.00/3.86G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   1% 52.4M/3.86G [00:00<00:08, 433MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   3% 105M/3.86G [00:00<00:07, 473MB/s] \u001b[A\n",
            "model-00002-of-00005.safetensors:   4% 157M/3.86G [00:00<00:07, 487MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   5% 210M/3.86G [00:00<00:07, 493MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   7% 262M/3.86G [00:00<00:07, 495MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   8% 315M/3.86G [00:00<00:07, 477MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:   9% 367M/3.86G [00:00<00:07, 468MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  11% 419M/3.86G [00:00<00:07, 459MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  12% 472M/3.86G [00:01<00:07, 446MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  14% 524M/3.86G [00:01<00:07, 424MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  15% 577M/3.86G [00:01<00:07, 427MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  16% 629M/3.86G [00:01<00:07, 425MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  18% 682M/3.86G [00:01<00:07, 419MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  19% 734M/3.86G [00:01<00:07, 422MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  20% 786M/3.86G [00:01<00:07, 436MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  22% 839M/3.86G [00:01<00:06, 439MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  23% 891M/3.86G [00:01<00:06, 447MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  24% 944M/3.86G [00:02<00:06, 460MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  26% 1.01G/3.86G [00:02<00:05, 483MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  27% 1.06G/3.86G [00:02<00:05, 488MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  29% 1.11G/3.86G [00:02<00:05, 483MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  30% 1.16G/3.86G [00:02<00:08, 304MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  31% 1.22G/3.86G [00:02<00:07, 337MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  33% 1.27G/3.86G [00:02<00:07, 361MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  34% 1.32G/3.86G [00:03<00:06, 384MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  36% 1.37G/3.86G [00:03<00:06, 403MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  37% 1.43G/3.86G [00:03<00:05, 414MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  38% 1.48G/3.86G [00:03<00:05, 422MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  40% 1.53G/3.86G [00:03<00:05, 425MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  41% 1.58G/3.86G [00:03<00:05, 433MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  42% 1.64G/3.86G [00:03<00:05, 435MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  44% 1.69G/3.86G [00:03<00:04, 454MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  45% 1.74G/3.86G [00:04<00:04, 464MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  46% 1.79G/3.86G [00:04<00:05, 390MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  48% 1.86G/3.86G [00:04<00:04, 427MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  49% 1.91G/3.86G [00:04<00:04, 445MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  51% 1.96G/3.86G [00:04<00:04, 459MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  52% 2.01G/3.86G [00:04<00:03, 471MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  53% 2.07G/3.86G [00:04<00:03, 478MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  55% 2.12G/3.86G [00:04<00:03, 485MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  56% 2.17G/3.86G [00:05<00:05, 337MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  58% 2.22G/3.86G [00:05<00:04, 367MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  59% 2.28G/3.86G [00:05<00:04, 393MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  60% 2.33G/3.86G [00:05<00:03, 414MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  62% 2.38G/3.86G [00:05<00:03, 374MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  63% 2.43G/3.86G [00:05<00:03, 392MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  64% 2.47G/3.86G [00:06<00:05, 269MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  65% 2.53G/3.86G [00:06<00:04, 316MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  67% 2.58G/3.86G [00:06<00:03, 348MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  68% 2.63G/3.86G [00:06<00:03, 373MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  69% 2.68G/3.86G [00:06<00:02, 398MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  71% 2.74G/3.86G [00:06<00:02, 415MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  72% 2.79G/3.86G [00:06<00:02, 406MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  74% 2.85G/3.86G [00:06<00:02, 440MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  75% 2.90G/3.86G [00:06<00:02, 456MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  77% 2.96G/3.86G [00:07<00:01, 454MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  78% 3.01G/3.86G [00:07<00:01, 462MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  79% 3.06G/3.86G [00:07<00:01, 470MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  81% 3.11G/3.86G [00:07<00:01, 480MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  82% 3.17G/3.86G [00:07<00:01, 441MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  83% 3.22G/3.86G [00:07<00:02, 282MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  85% 3.27G/3.86G [00:08<00:01, 316MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  86% 3.32G/3.86G [00:08<00:01, 343MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  87% 3.37G/3.86G [00:08<00:01, 358MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  88% 3.42G/3.86G [00:08<00:01, 377MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  90% 3.47G/3.86G [00:08<00:01, 394MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  91% 3.52G/3.86G [00:08<00:00, 395MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  93% 3.58G/3.86G [00:08<00:00, 408MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  94% 3.63G/3.86G [00:08<00:00, 422MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  95% 3.68G/3.86G [00:08<00:00, 432MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  97% 3.73G/3.86G [00:09<00:00, 296MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  98% 3.77G/3.86G [00:09<00:00, 287MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors:  99% 3.82G/3.86G [00:09<00:00, 286MB/s]\u001b[A\n",
            "model-00002-of-00005.safetensors: 100% 3.86G/3.86G [00:09<00:00, 395MB/s]\n",
            "Downloading shards:  40% 2/5 [00:20<00:31, 10.34s/it]\n",
            "model-00003-of-00005.safetensors:   0% 0.00/3.86G [00:00<?, ?B/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   1% 21.0M/3.86G [00:00<00:22, 170MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   1% 52.4M/3.86G [00:00<00:16, 232MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   3% 105M/3.86G [00:00<00:11, 340MB/s] \u001b[A\n",
            "model-00003-of-00005.safetensors:   4% 157M/3.86G [00:00<00:09, 401MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   5% 210M/3.86G [00:00<00:08, 436MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   7% 262M/3.86G [00:00<00:08, 434MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   8% 315M/3.86G [00:00<00:10, 342MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:   9% 367M/3.86G [00:00<00:09, 382MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  11% 419M/3.86G [00:01<00:08, 405MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  12% 472M/3.86G [00:01<00:07, 426MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  14% 524M/3.86G [00:01<00:07, 441MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  15% 577M/3.86G [00:01<00:07, 455MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  16% 629M/3.86G [00:01<00:06, 464MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  18% 682M/3.86G [00:01<00:06, 471MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  19% 734M/3.86G [00:01<00:06, 474MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  20% 786M/3.86G [00:01<00:06, 481MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  22% 839M/3.86G [00:01<00:06, 483MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  23% 891M/3.86G [00:02<00:06, 486MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  25% 954M/3.86G [00:02<00:05, 500MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  26% 1.02G/3.86G [00:02<00:05, 509MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  28% 1.07G/3.86G [00:02<00:05, 513MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  29% 1.13G/3.86G [00:02<00:05, 518MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  31% 1.18G/3.86G [00:02<00:05, 518MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  32% 1.24G/3.86G [00:02<00:05, 513MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  33% 1.29G/3.86G [00:02<00:05, 512MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  35% 1.34G/3.86G [00:02<00:04, 505MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  36% 1.39G/3.86G [00:03<00:04, 507MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  37% 1.45G/3.86G [00:03<00:04, 501MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  39% 1.50G/3.86G [00:03<00:04, 496MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  40% 1.55G/3.86G [00:03<00:05, 406MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  42% 1.60G/3.86G [00:03<00:05, 389MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  43% 1.66G/3.86G [00:03<00:05, 401MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  44% 1.70G/3.86G [00:03<00:05, 379MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  45% 1.74G/3.86G [00:03<00:05, 387MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  46% 1.78G/3.86G [00:04<00:05, 388MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  47% 1.82G/3.86G [00:04<00:06, 328MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  49% 1.88G/3.86G [00:04<00:05, 356MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  50% 1.92G/3.86G [00:04<00:06, 287MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  51% 1.96G/3.86G [00:04<00:06, 314MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  52% 2.00G/3.86G [00:04<00:07, 250MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  53% 2.04G/3.86G [00:05<00:06, 282MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  54% 2.10G/3.86G [00:05<00:05, 321MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  56% 2.15G/3.86G [00:05<00:04, 352MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  57% 2.20G/3.86G [00:05<00:04, 373MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  58% 2.25G/3.86G [00:05<00:04, 388MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  60% 2.31G/3.86G [00:05<00:03, 404MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  61% 2.36G/3.86G [00:05<00:03, 415MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  62% 2.41G/3.86G [00:05<00:03, 417MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  64% 2.46G/3.86G [00:06<00:03, 427MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  65% 2.52G/3.86G [00:06<00:02, 450MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  66% 2.57G/3.86G [00:06<00:02, 457MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  68% 2.62G/3.86G [00:06<00:02, 454MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  69% 2.67G/3.86G [00:06<00:02, 468MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  71% 2.74G/3.86G [00:06<00:02, 486MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  72% 2.79G/3.86G [00:06<00:02, 369MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  73% 2.83G/3.86G [00:06<00:03, 339MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  74% 2.87G/3.86G [00:07<00:03, 316MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  75% 2.92G/3.86G [00:07<00:02, 319MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  77% 2.96G/3.86G [00:07<00:02, 317MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  78% 3.00G/3.86G [00:07<00:02, 319MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  79% 3.04G/3.86G [00:07<00:02, 305MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  79% 3.07G/3.86G [00:07<00:03, 213MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  81% 3.12G/3.86G [00:08<00:02, 270MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  82% 3.19G/3.86G [00:08<00:02, 318MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  84% 3.23G/3.86G [00:08<00:02, 292MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  85% 3.27G/3.86G [00:08<00:02, 278MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  85% 3.30G/3.86G [00:08<00:02, 262MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  86% 3.33G/3.86G [00:08<00:02, 248MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  87% 3.37G/3.86G [00:08<00:02, 233MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  88% 3.40G/3.86G [00:09<00:02, 223MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  89% 3.44G/3.86G [00:09<00:01, 259MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  90% 3.49G/3.86G [00:09<00:01, 318MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  92% 3.54G/3.86G [00:09<00:00, 366MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  93% 3.60G/3.86G [00:09<00:00, 397MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  94% 3.64G/3.86G [00:09<00:00, 388MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  95% 3.68G/3.86G [00:09<00:00, 377MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  97% 3.73G/3.86G [00:09<00:00, 400MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  98% 3.77G/3.86G [00:17<00:04, 20.5MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors:  98% 3.81G/3.86G [00:17<00:02, 25.9MB/s]\u001b[A\n",
            "model-00003-of-00005.safetensors: 100% 3.86G/3.86G [00:17<00:00, 223MB/s] \n",
            "Downloading shards:  60% 3/5 [00:38<00:27, 13.76s/it]\n",
            "model-00004-of-00005.safetensors:   0% 0.00/3.86G [00:00<?, ?B/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   1% 52.4M/3.86G [00:00<00:08, 447MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   3% 105M/3.86G [00:00<00:08, 454MB/s] \u001b[A\n",
            "model-00004-of-00005.safetensors:   4% 157M/3.86G [00:00<00:08, 456MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   5% 210M/3.86G [00:00<00:08, 456MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   7% 262M/3.86G [00:00<00:07, 457MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   8% 315M/3.86G [00:00<00:07, 459MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:   9% 367M/3.86G [00:00<00:07, 455MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  11% 419M/3.86G [00:00<00:07, 452MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  12% 472M/3.86G [00:01<00:11, 290MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  14% 524M/3.86G [00:01<00:10, 323MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  15% 577M/3.86G [00:01<00:09, 346MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  16% 629M/3.86G [00:01<00:08, 369MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  18% 682M/3.86G [00:01<00:08, 390MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  19% 734M/3.86G [00:01<00:07, 405MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  20% 786M/3.86G [00:01<00:07, 420MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  22% 839M/3.86G [00:02<00:08, 347MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  23% 891M/3.86G [00:02<00:07, 374MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  24% 944M/3.86G [00:02<00:07, 397MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  26% 996M/3.86G [00:02<00:06, 415MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  27% 1.05G/3.86G [00:02<00:06, 426MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  28% 1.10G/3.86G [00:02<00:06, 436MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  30% 1.15G/3.86G [00:02<00:06, 443MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  31% 1.21G/3.86G [00:02<00:05, 449MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  33% 1.26G/3.86G [00:03<00:05, 454MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  34% 1.31G/3.86G [00:03<00:06, 416MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  35% 1.36G/3.86G [00:03<00:06, 360MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  36% 1.41G/3.86G [00:03<00:07, 327MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  37% 1.45G/3.86G [00:03<00:07, 331MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  39% 1.49G/3.86G [00:03<00:06, 345MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  40% 1.53G/3.86G [00:03<00:06, 340MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  41% 1.57G/3.86G [00:04<00:06, 338MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  42% 1.61G/3.86G [00:04<00:07, 320MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  43% 1.66G/3.86G [00:04<00:07, 314MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  44% 1.70G/3.86G [00:04<00:06, 330MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  45% 1.74G/3.86G [00:04<00:06, 332MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  46% 1.78G/3.86G [00:04<00:06, 326MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  47% 1.82G/3.86G [00:04<00:06, 338MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  49% 1.88G/3.86G [00:04<00:05, 364MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  50% 1.93G/3.86G [00:05<00:04, 387MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  51% 1.98G/3.86G [00:05<00:04, 407MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  53% 2.03G/3.86G [00:05<00:04, 423MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  54% 2.09G/3.86G [00:05<00:04, 433MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  55% 2.14G/3.86G [00:05<00:04, 410MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  56% 2.18G/3.86G [00:05<00:04, 403MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  58% 2.22G/3.86G [00:05<00:04, 390MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  59% 2.26G/3.86G [00:05<00:04, 379MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  60% 2.31G/3.86G [00:06<00:04, 362MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  61% 2.35G/3.86G [00:06<00:04, 347MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  62% 2.39G/3.86G [00:06<00:04, 328MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  63% 2.43G/3.86G [00:06<00:04, 326MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  64% 2.47G/3.86G [00:06<00:04, 323MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  65% 2.52G/3.86G [00:06<00:04, 321MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  66% 2.56G/3.86G [00:06<00:03, 344MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  68% 2.61G/3.86G [00:06<00:03, 373MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  69% 2.66G/3.86G [00:07<00:03, 358MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  70% 2.71G/3.86G [00:07<00:04, 273MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  71% 2.76G/3.86G [00:07<00:03, 310MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  73% 2.81G/3.86G [00:07<00:03, 340MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  74% 2.85G/3.86G [00:07<00:02, 350MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  75% 2.89G/3.86G [00:07<00:02, 355MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  76% 2.94G/3.86G [00:07<00:02, 340MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  77% 2.98G/3.86G [00:08<00:03, 296MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  78% 3.01G/3.86G [00:08<00:03, 258MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  79% 3.06G/3.86G [00:08<00:02, 304MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  81% 3.11G/3.86G [00:08<00:02, 347MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  82% 3.17G/3.86G [00:08<00:01, 378MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  83% 3.22G/3.86G [00:08<00:01, 400MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  85% 3.27G/3.86G [00:08<00:01, 412MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  86% 3.32G/3.86G [00:08<00:01, 430MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  87% 3.38G/3.86G [00:09<00:01, 438MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  89% 3.43G/3.86G [00:09<00:01, 386MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  90% 3.47G/3.86G [00:09<00:01, 385MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  91% 3.51G/3.86G [00:09<00:00, 384MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  92% 3.55G/3.86G [00:09<00:00, 376MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  93% 3.61G/3.86G [00:09<00:00, 389MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  94% 3.65G/3.86G [00:09<00:00, 363MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  96% 3.70G/3.86G [00:10<00:00, 385MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  97% 3.74G/3.86G [00:10<00:00, 376MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors:  98% 3.80G/3.86G [00:10<00:00, 395MB/s]\u001b[A\n",
            "model-00004-of-00005.safetensors: 100% 3.86G/3.86G [00:10<00:00, 371MB/s]\n",
            "Downloading shards:  80% 4/5 [00:49<00:12, 12.62s/it]\n",
            "model-00005-of-00005.safetensors:   0% 0.00/1.09G [00:00<?, ?B/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:   5% 52.4M/1.09G [00:00<00:02, 467MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  10% 105M/1.09G [00:00<00:02, 462MB/s] \u001b[A\n",
            "model-00005-of-00005.safetensors:  14% 157M/1.09G [00:00<00:02, 458MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  19% 210M/1.09G [00:00<00:01, 457MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  24% 262M/1.09G [00:00<00:01, 457MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  29% 315M/1.09G [00:00<00:01, 454MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  34% 367M/1.09G [00:00<00:01, 439MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  38% 419M/1.09G [00:01<00:02, 323MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  42% 461M/1.09G [00:01<00:01, 320MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  46% 503M/1.09G [00:01<00:02, 230MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  51% 556M/1.09G [00:01<00:01, 276MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  56% 608M/1.09G [00:01<00:01, 315MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  60% 650M/1.09G [00:01<00:01, 305MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  63% 692M/1.09G [00:02<00:01, 277MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  66% 724M/1.09G [00:02<00:01, 263MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  69% 755M/1.09G [00:02<00:01, 247MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  72% 786M/1.09G [00:02<00:01, 253MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  75% 818M/1.09G [00:02<00:01, 260MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  79% 860M/1.09G [00:02<00:00, 279MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  83% 902M/1.09G [00:02<00:00, 306MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  88% 954M/1.09G [00:02<00:00, 358MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors:  92% 1.01G/1.09G [00:03<00:00, 395MB/s]\u001b[A\n",
            "model-00005-of-00005.safetensors: 100% 1.09G/1.09G [00:03<00:00, 337MB/s]\n",
            "Downloading shards: 100% 5/5 [00:53<00:00, 10.63s/it]\n",
            "[INFO|modeling_utils.py:1613] 2024-09-15 19:52:50,930 >> Instantiating Qwen2VLForConditionalGeneration model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1097] 2024-09-15 19:52:50,931 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2024-09-15 19:52:50,955 >> `Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
            "Loading checkpoint shards: 100% 5/5 [00:06<00:00,  1.33s/it]\n",
            "[INFO|modeling_utils.py:4526] 2024-09-15 19:52:57,774 >> All model checkpoint weights were used when initializing Qwen2VLForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4534] 2024-09-15 19:52:57,774 >> All the weights of Qwen2VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2-VL-7B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2VLForConditionalGeneration for predictions without further training.\n",
            "generation_config.json: 100% 244/244 [00:00<00:00, 1.75MB/s]\n",
            "[INFO|configuration_utils.py:1052] 2024-09-15 19:52:58,250 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/generation_config.json\n",
            "[INFO|configuration_utils.py:1097] 2024-09-15 19:52:58,250 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"temperature\": 0.01,\n",
            "  \"top_k\": 1,\n",
            "  \"top_p\": 0.001\n",
            "}\n",
            "\n",
            "09/15/2024 19:52:58 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "09/15/2024 19:52:58 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "09/15/2024 19:52:58 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "09/15/2024 19:52:58 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "09/15/2024 19:52:58 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,down_proj,v_proj,q_proj,gate_proj,k_proj,o_proj\n",
            "09/15/2024 19:52:59 - INFO - llamafactory.model.loader - trainable params: 20,185,088 || all params: 8,311,560,704 || trainable%: 0.2429\n",
            "[INFO|trainer.py:668] 2024-09-15 19:52:59,491 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2213] 2024-09-15 19:52:59,936 >> ***** Running training *****\n",
            "[INFO|trainer.py:2214] 2024-09-15 19:52:59,936 >>   Num examples = 900\n",
            "[INFO|trainer.py:2215] 2024-09-15 19:52:59,936 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2216] 2024-09-15 19:52:59,936 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2219] 2024-09-15 19:52:59,936 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2220] 2024-09-15 19:52:59,936 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2221] 2024-09-15 19:52:59,936 >>   Total optimization steps = 336\n",
            "[INFO|trainer.py:2222] 2024-09-15 19:52:59,943 >>   Number of trainable parameters = 20,185,088\n",
            "{'loss': 0.6964, 'grad_norm': 1.327682375907898, 'learning_rate': 1.4705882352941177e-06, 'rewards/chosen': -0.0014081653207540512, 'rewards/rejected': 0.004795796703547239, 'rewards/accuracies': 0.3125, 'rewards/margins': -0.006203961558640003, 'logps/rejected': -124.34664154052734, 'logps/chosen': -150.91493225097656, 'logits/rejected': -2.5056159496307373, 'logits/chosen': -2.4643263816833496, 'epoch': 0.09}\n",
            "{'loss': 0.6945, 'grad_norm': 1.6093658208847046, 'learning_rate': 2.9411764705882355e-06, 'rewards/chosen': -0.0006540458416566253, 'rewards/rejected': 0.0015441288705915213, 'rewards/accuracies': 0.4375, 'rewards/margins': -0.002198175759986043, 'logps/rejected': -165.11624145507812, 'logps/chosen': -180.83702087402344, 'logits/rejected': -2.5269761085510254, 'logits/chosen': -2.501800060272217, 'epoch': 0.18}\n",
            "{'loss': 0.692, 'grad_norm': 2.0969972610473633, 'learning_rate': 4.411764705882353e-06, 'rewards/chosen': 0.0019743002485483885, 'rewards/rejected': -0.0009272711467929184, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0029015715699642897, 'logps/rejected': -140.18899536132812, 'logps/chosen': -168.0308380126953, 'logits/rejected': -2.481177568435669, 'logits/chosen': -2.422253370285034, 'epoch': 0.27}\n",
            "{'loss': 0.6954, 'grad_norm': 1.9670945405960083, 'learning_rate': 4.995131923687488e-06, 'rewards/chosen': 0.0003977052983827889, 'rewards/rejected': 0.004468409810215235, 'rewards/accuracies': 0.4375, 'rewards/margins': -0.004070704337209463, 'logps/rejected': -133.6790313720703, 'logps/chosen': -151.64634704589844, 'logits/rejected': -2.457615375518799, 'logits/chosen': -2.4039299488067627, 'epoch': 0.36}\n",
            "{'loss': 0.6899, 'grad_norm': 1.015565037727356, 'learning_rate': 4.965451197130373e-06, 'rewards/chosen': 0.007453619036823511, 'rewards/rejected': 5.6091790611390024e-05, 'rewards/accuracies': 0.550000011920929, 'rewards/margins': 0.00739752734079957, 'logps/rejected': -128.85162353515625, 'logps/chosen': -147.65151977539062, 'logits/rejected': -2.482834577560425, 'logits/chosen': -2.444418430328369, 'epoch': 0.44}\n",
            "{'loss': 0.6839, 'grad_norm': 2.2965869903564453, 'learning_rate': 4.90911473983908e-06, 'rewards/chosen': 0.011699984781444073, 'rewards/rejected': -0.007330249063670635, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.019030233845114708, 'logps/rejected': -128.37393188476562, 'logps/chosen': -143.77130126953125, 'logits/rejected': -2.508744478225708, 'logits/chosen': -2.4718241691589355, 'epoch': 0.53}\n",
            "{'loss': 0.6849, 'grad_norm': 1.7350943088531494, 'learning_rate': 4.826731644963705e-06, 'rewards/chosen': 0.013431603088974953, 'rewards/rejected': -0.0040480210445821285, 'rewards/accuracies': 0.625, 'rewards/margins': 0.01747962273657322, 'logps/rejected': -143.26100158691406, 'logps/chosen': -162.09243774414062, 'logits/rejected': -2.4680368900299072, 'logits/chosen': -2.4290690422058105, 'epoch': 0.62}\n",
            "{'loss': 0.6866, 'grad_norm': 1.501512050628662, 'learning_rate': 4.71919261421297e-06, 'rewards/chosen': 0.010992015711963177, 'rewards/rejected': -0.0028084698133170605, 'rewards/accuracies': 0.5375000238418579, 'rewards/margins': 0.01380048505961895, 'logps/rejected': -122.822265625, 'logps/chosen': -133.4914093017578, 'logits/rejected': -2.395512580871582, 'logits/chosen': -2.3626372814178467, 'epoch': 0.71}\n",
            "{'loss': 0.6764, 'grad_norm': 1.545676589012146, 'learning_rate': 4.587660327850203e-06, 'rewards/chosen': 0.015342813916504383, 'rewards/rejected': -0.01992916315793991, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.03527197986841202, 'logps/rejected': -122.10643005371094, 'logps/chosen': -140.13352966308594, 'logits/rejected': -2.463059186935425, 'logits/chosen': -2.4071106910705566, 'epoch': 0.8}\n",
            "{'loss': 0.671, 'grad_norm': 2.0184497833251953, 'learning_rate': 4.43355687413747e-06, 'rewards/chosen': 0.04344571754336357, 'rewards/rejected': -0.0033436245284974575, 'rewards/accuracies': 0.7749999761581421, 'rewards/margins': 0.046789344400167465, 'logps/rejected': -119.96763610839844, 'logps/chosen': -135.64511108398438, 'logits/rejected': -2.4566969871520996, 'logits/chosen': -2.403136968612671, 'epoch': 0.89}\n",
            "{'loss': 0.6611, 'grad_norm': 1.9681224822998047, 'learning_rate': 4.258548374136976e-06, 'rewards/chosen': 0.07706092298030853, 'rewards/rejected': 0.008375993929803371, 'rewards/accuracies': 0.699999988079071, 'rewards/margins': 0.06868493556976318, 'logps/rejected': -145.93612670898438, 'logps/chosen': -160.11724853515625, 'logits/rejected': -2.459501266479492, 'logits/chosen': -2.4191880226135254, 'epoch': 0.98}\n",
            "{'loss': 0.6446, 'grad_norm': 1.1570279598236084, 'learning_rate': 4.064526968101844e-06, 'rewards/chosen': 0.10261861234903336, 'rewards/rejected': -0.0035751566756516695, 'rewards/accuracies': 0.824999988079071, 'rewards/margins': 0.10619376599788666, 'logps/rejected': -123.06683349609375, 'logps/chosen': -141.5924835205078, 'logits/rejected': -2.483445644378662, 'logits/chosen': -2.4429001808166504, 'epoch': 1.07}\n",
            "{'loss': 0.6172, 'grad_norm': 1.8025197982788086, 'learning_rate': 3.853590358214119e-06, 'rewards/chosen': 0.11447791010141373, 'rewards/rejected': -0.052106358110904694, 'rewards/accuracies': 0.8999999761581421, 'rewards/margins': 0.16658425331115723, 'logps/rejected': -133.81231689453125, 'logps/chosen': -151.16307067871094, 'logits/rejected': -2.4528541564941406, 'logits/chosen': -2.409158229827881, 'epoch': 1.16}\n",
            "{'loss': 0.6187, 'grad_norm': 2.2242391109466553, 'learning_rate': 3.6280191288478437e-06, 'rewards/chosen': 0.09068737179040909, 'rewards/rejected': -0.07689972966909409, 'rewards/accuracies': 0.887499988079071, 'rewards/margins': 0.16758711636066437, 'logps/rejected': -109.38460540771484, 'logps/chosen': -123.74504089355469, 'logits/rejected': -2.539865016937256, 'logits/chosen': -2.4978690147399902, 'epoch': 1.24}\n",
            "{'loss': 0.586, 'grad_norm': 2.1713790893554688, 'learning_rate': 3.3902520895638674e-06, 'rewards/chosen': 0.21727874875068665, 'rewards/rejected': -0.025097046047449112, 'rewards/accuracies': 0.9125000238418579, 'rewards/margins': 0.24237577617168427, 'logps/rejected': -146.56338500976562, 'logps/chosen': -158.86485290527344, 'logits/rejected': -2.4354910850524902, 'logits/chosen': -2.407068967819214, 'epoch': 1.33}\n",
            "{'loss': 0.5927, 'grad_norm': 1.4953943490982056, 'learning_rate': 3.142859907420615e-06, 'rewards/chosen': 0.25561225414276123, 'rewards/rejected': 0.02275703474879265, 'rewards/accuracies': 0.862500011920929, 'rewards/margins': 0.2328551709651947, 'logps/rejected': -145.41891479492188, 'logps/chosen': -161.75106811523438, 'logits/rejected': -2.489938974380493, 'logits/chosen': -2.4399075508117676, 'epoch': 1.42}\n",
            "{'loss': 0.6079, 'grad_norm': 2.0755879878997803, 'learning_rate': 2.8885173136805126e-06, 'rewards/chosen': 0.13242729008197784, 'rewards/rejected': -0.06517347693443298, 'rewards/accuracies': 0.8125, 'rewards/margins': 0.19760073721408844, 'logps/rejected': -132.3262176513672, 'logps/chosen': -146.48483276367188, 'logits/rejected': -2.5361921787261963, 'logits/chosen': -2.49049973487854, 'epoch': 1.51}\n",
            "{'loss': 0.585, 'grad_norm': 2.1435821056365967, 'learning_rate': 2.629974185404951e-06, 'rewards/chosen': 0.19610467553138733, 'rewards/rejected': -0.06715508550405502, 'rewards/accuracies': 0.8999999761581421, 'rewards/margins': 0.26325979828834534, 'logps/rejected': -136.92181396484375, 'logps/chosen': -150.03280639648438, 'logits/rejected': -2.511956214904785, 'logits/chosen': -2.4619359970092773, 'epoch': 1.6}\n",
            "{'loss': 0.5911, 'grad_norm': 1.5959124565124512, 'learning_rate': 2.3700258145950495e-06, 'rewards/chosen': 0.21892455220222473, 'rewards/rejected': -0.031018083915114403, 'rewards/accuracies': 0.762499988079071, 'rewards/margins': 0.2499426156282425, 'logps/rejected': -127.5325927734375, 'logps/chosen': -146.2853240966797, 'logits/rejected': -2.473144054412842, 'logits/chosen': -2.443798303604126, 'epoch': 1.69}\n",
            "{'loss': 0.5833, 'grad_norm': 1.6481093168258667, 'learning_rate': 2.1114826863194882e-06, 'rewards/chosen': 0.2671136260032654, 'rewards/rejected': 0.00542424526065588, 'rewards/accuracies': 0.8374999761581421, 'rewards/margins': 0.26168936491012573, 'logps/rejected': -135.42282104492188, 'logps/chosen': -154.25967407226562, 'logits/rejected': -2.6207408905029297, 'logits/chosen': -2.5813963413238525, 'epoch': 1.78}\n",
            "{'loss': 0.5751, 'grad_norm': 1.780358076095581, 'learning_rate': 1.8571400925793855e-06, 'rewards/chosen': 0.20243306457996368, 'rewards/rejected': -0.07915247231721878, 'rewards/accuracies': 0.862500011920929, 'rewards/margins': 0.2815855145454407, 'logps/rejected': -126.68812561035156, 'logps/chosen': -138.43971252441406, 'logits/rejected': -2.496579647064209, 'logits/chosen': -2.4397542476654053, 'epoch': 1.87}\n",
            "{'loss': 0.5492, 'grad_norm': 1.8811115026474, 'learning_rate': 1.6097479104361328e-06, 'rewards/chosen': 0.2089575231075287, 'rewards/rejected': -0.1503082811832428, 'rewards/accuracies': 0.862500011920929, 'rewards/margins': 0.3592657744884491, 'logps/rejected': -153.12940979003906, 'logps/chosen': -170.76414489746094, 'logits/rejected': -2.500086784362793, 'logits/chosen': -2.4749255180358887, 'epoch': 1.96}\n",
            "{'loss': 0.5703, 'grad_norm': 1.6730767488479614, 'learning_rate': 1.3719808711521573e-06, 'rewards/chosen': 0.2403869330883026, 'rewards/rejected': -0.06687958538532257, 'rewards/accuracies': 0.7749999761581421, 'rewards/margins': 0.307266503572464, 'logps/rejected': -110.8914794921875, 'logps/chosen': -123.46331787109375, 'logits/rejected': -2.5539863109588623, 'logits/chosen': -2.4935595989227295, 'epoch': 2.04}\n",
            "{'loss': 0.5252, 'grad_norm': 1.7572985887527466, 'learning_rate': 1.1464096417858821e-06, 'rewards/chosen': 0.2925841212272644, 'rewards/rejected': -0.13314016163349152, 'rewards/accuracies': 0.875, 'rewards/margins': 0.4257243573665619, 'logps/rejected': -132.89132690429688, 'logps/chosen': -148.93508911132812, 'logits/rejected': -2.555518388748169, 'logits/chosen': -2.5141520500183105, 'epoch': 2.13}\n",
            "{'loss': 0.5414, 'grad_norm': 2.018859624862671, 'learning_rate': 9.354730318981561e-07, 'rewards/chosen': 0.2760133743286133, 'rewards/rejected': -0.11929526180028915, 'rewards/accuracies': 0.887499988079071, 'rewards/margins': 0.39530861377716064, 'logps/rejected': -150.9976348876953, 'logps/chosen': -167.46044921875, 'logits/rejected': -2.465014696121216, 'logits/chosen': -2.429861068725586, 'epoch': 2.22}\n",
            "{'loss': 0.5632, 'grad_norm': 1.5614423751831055, 'learning_rate': 7.414516258630245e-07, 'rewards/chosen': 0.21694020926952362, 'rewards/rejected': -0.09021560847759247, 'rewards/accuracies': 0.862500011920929, 'rewards/margins': 0.3071558475494385, 'logps/rejected': -134.56712341308594, 'logps/chosen': -151.4268798828125, 'logits/rejected': -2.485734462738037, 'logits/chosen': -2.439937114715576, 'epoch': 2.31}\n",
            "{'loss': 0.5491, 'grad_norm': 1.6416889429092407, 'learning_rate': 5.664431258625305e-07, 'rewards/chosen': 0.3427683115005493, 'rewards/rejected': -0.035240255296230316, 'rewards/accuracies': 0.8125, 'rewards/margins': 0.37800854444503784, 'logps/rejected': -127.85941314697266, 'logps/chosen': -143.76194763183594, 'logits/rejected': -2.480332851409912, 'logits/chosen': -2.429975986480713, 'epoch': 2.4}\n",
            "{'loss': 0.5056, 'grad_norm': 1.8668172359466553, 'learning_rate': 4.123396721497977e-07, 'rewards/chosen': 0.2892840504646301, 'rewards/rejected': -0.20517535507678986, 'rewards/accuracies': 0.9125000238418579, 'rewards/margins': 0.4944593906402588, 'logps/rejected': -149.61953735351562, 'logps/chosen': -164.58633422851562, 'logits/rejected': -2.510638475418091, 'logits/chosen': -2.456357002258301, 'epoch': 2.49}\n",
            "{'loss': 0.5317, 'grad_norm': 2.0457730293273926, 'learning_rate': 2.8080738578703054e-07, 'rewards/chosen': 0.17870743572711945, 'rewards/rejected': -0.2387632429599762, 'rewards/accuracies': 0.8999999761581421, 'rewards/margins': 0.41747063398361206, 'logps/rejected': -134.94805908203125, 'logps/chosen': -146.6786651611328, 'logits/rejected': -2.572758197784424, 'logits/chosen': -2.5380187034606934, 'epoch': 2.58}\n",
            "{'loss': 0.5296, 'grad_norm': 1.706403374671936, 'learning_rate': 1.7326835503629542e-07, 'rewards/chosen': 0.2922517657279968, 'rewards/rejected': -0.12536413967609406, 'rewards/accuracies': 0.8999999761581421, 'rewards/margins': 0.41761595010757446, 'logps/rejected': -122.43585205078125, 'logps/chosen': -140.9709014892578, 'logits/rejected': -2.5015921592712402, 'logits/chosen': -2.44880747795105, 'epoch': 2.67}\n",
            "{'loss': 0.5566, 'grad_norm': 2.0176594257354736, 'learning_rate': 9.088526016092142e-08, 'rewards/chosen': 0.2535814642906189, 'rewards/rejected': -0.08233614265918732, 'rewards/accuracies': 0.875, 'rewards/margins': 0.3359176516532898, 'logps/rejected': -122.6108169555664, 'logps/chosen': -134.37391662597656, 'logits/rejected': -2.548340320587158, 'logits/chosen': -2.4907686710357666, 'epoch': 2.76}\n",
            "{'loss': 0.504, 'grad_norm': 1.6555795669555664, 'learning_rate': 3.4548802869627806e-08, 'rewards/chosen': 0.3783073127269745, 'rewards/rejected': -0.09686659276485443, 'rewards/accuracies': 0.9375, 'rewards/margins': 0.4751739501953125, 'logps/rejected': -155.92318725585938, 'logps/chosen': -169.57015991210938, 'logits/rejected': -2.5828351974487305, 'logits/chosen': -2.5243465900421143, 'epoch': 2.84}\n",
            "{'loss': 0.543, 'grad_norm': 1.5940535068511963, 'learning_rate': 4.868076312512515e-09, 'rewards/chosen': 0.29395145177841187, 'rewards/rejected': -0.11211223900318146, 'rewards/accuracies': 0.7875000238418579, 'rewards/margins': 0.4060637056827545, 'logps/rejected': -147.59280395507812, 'logps/chosen': -161.06973266601562, 'logits/rejected': -2.5172276496887207, 'logits/chosen': -2.480404853820801, 'epoch': 2.93}\n",
            "100% 336/336 [1:12:40<00:00, 12.21s/it][INFO|trainer.py:3675] 2024-09-15 21:05:40,484 >> Saving model checkpoint to /conten//drive/MyDrive/saves/qwen2_vl-7b/lora/dpo/checkpoint-336\n",
            "[INFO|configuration_utils.py:672] 2024-09-15 21:05:41,039 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/config.json\n",
            "[WARNING|modeling_rope_utils.py:379] 2024-09-15 21:05:41,039 >> Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
            "[INFO|configuration_utils.py:739] 2024-09-15 21:05:41,041 >> Model config Qwen2VLConfig {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2_vl\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.45.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"in_chans\": 3,\n",
            "    \"model_type\": \"qwen2_vl\",\n",
            "    \"spatial_patch_size\": 14\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2650] 2024-09-15 21:05:41,208 >> tokenizer config file saved in /conten//drive/MyDrive/saves/qwen2_vl-7b/lora/dpo/checkpoint-336/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2659] 2024-09-15 21:05:41,209 >> Special tokens file saved in /conten//drive/MyDrive/saves/qwen2_vl-7b/lora/dpo/checkpoint-336/special_tokens_map.json\n",
            "[INFO|image_processing_base.py:258] 2024-09-15 21:05:41,709 >> Image processor saved in /conten//drive/MyDrive/saves/qwen2_vl-7b/lora/dpo/checkpoint-336/preprocessor_config.json\n",
            "[INFO|trainer.py:2475] 2024-09-15 21:05:41,709 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4361.7664, 'train_samples_per_second': 0.619, 'train_steps_per_second': 0.077, 'train_loss': 0.6054522047440211, 'epoch': 2.99}\n",
            "100% 336/336 [1:12:41<00:00, 12.98s/it]\n",
            "[INFO|image_processing_base.py:258] 2024-09-15 21:05:41,712 >> Image processor saved in /conten//drive/MyDrive/saves/qwen2_vl-7b/lora/dpo/preprocessor_config.json\n",
            "[INFO|trainer.py:3675] 2024-09-15 21:05:41,712 >> Saving model checkpoint to /conten//drive/MyDrive/saves/qwen2_vl-7b/lora/dpo\n",
            "[INFO|configuration_utils.py:672] 2024-09-15 21:05:42,201 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/3ca981c995b0ce691d85d8408216da11ff92f690/config.json\n",
            "[WARNING|modeling_rope_utils.py:379] 2024-09-15 21:05:42,202 >> Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
            "[INFO|configuration_utils.py:739] 2024-09-15 21:05:42,203 >> Model config Qwen2VLConfig {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2VLForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"image_token_id\": 151655,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2_vl\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": {\n",
            "    \"mrope_section\": [\n",
            "      16,\n",
            "      24,\n",
            "      24\n",
            "    ],\n",
            "    \"rope_type\": \"default\",\n",
            "    \"type\": \"default\"\n",
            "  },\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.45.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"video_token_id\": 151656,\n",
            "  \"vision_config\": {\n",
            "    \"in_chans\": 3,\n",
            "    \"model_type\": \"qwen2_vl\",\n",
            "    \"spatial_patch_size\": 14\n",
            "  },\n",
            "  \"vision_end_token_id\": 151653,\n",
            "  \"vision_start_token_id\": 151652,\n",
            "  \"vision_token_id\": 151654,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2650] 2024-09-15 21:05:42,388 >> tokenizer config file saved in /conten//drive/MyDrive/saves/qwen2_vl-7b/lora/dpo/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2659] 2024-09-15 21:05:42,388 >> Special tokens file saved in /conten//drive/MyDrive/saves/qwen2_vl-7b/lora/dpo/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =     2.9867\n",
            "  total_flos               = 78842541GF\n",
            "  train_loss               =     0.6055\n",
            "  train_runtime            = 1:12:41.76\n",
            "  train_samples_per_second =      0.619\n",
            "  train_steps_per_second   =      0.077\n",
            "Figure saved at: /conten//drive/MyDrive/saves/qwen2_vl-7b/lora/dpo/training_loss.png\n",
            "09/15/2024 21:05:42 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.\n",
            "Figure saved at: /conten//drive/MyDrive/saves/qwen2_vl-7b/lora/dpo/training_rewards_accuracies.png\n",
            "[INFO|trainer.py:3991] 2024-09-15 21:05:43,011 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3993] 2024-09-15 21:05:43,012 >>   Num examples = 100\n",
            "[INFO|trainer.py:3996] 2024-09-15 21:05:43,012 >>   Batch size = 1\n",
            "100% 100/100 [01:37<00:00,  1.03it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =     2.9867\n",
            "  eval_logits/chosen      =    -2.4405\n",
            "  eval_logits/rejected    =    -2.4862\n",
            "  eval_logps/chosen       =  -144.3718\n",
            "  eval_logps/rejected     =  -121.4606\n",
            "  eval_loss               =     0.5441\n",
            "  eval_rewards/accuracies =       0.88\n",
            "  eval_rewards/chosen     =     0.3006\n",
            "  eval_rewards/margins    =     0.3725\n",
            "  eval_rewards/rejected   =    -0.0718\n",
            "  eval_runtime            = 0:01:37.99\n",
            "  eval_samples_per_second =       1.02\n",
            "  eval_steps_per_second   =       1.02\n",
            "[INFO|modelcard.py:449] 2024-09-15 21:07:21,015 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference using fine tuned model"
      ],
      "metadata": {
        "id": "h1FqVkDvQkx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/drive/MyDrive/cmpe_297_assignmet2/LLaMA-Factory\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "  adapter_name_or_path=\"/conten/drive/MyDrive/saves/llam3/ppo\",            # load the saved LoRA adapters\n",
        "  template=\"llama3\",                     # same to the one in training\n",
        "  finetuning_type=\"lora\",                  # same to the one in training\n",
        "  quantization_bit=4,                    # load 4-bit quantized model\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"History has been removed.\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ],
      "metadata": {
        "id": "9lDNyUF_PSsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WLPpEqBsg1xM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}