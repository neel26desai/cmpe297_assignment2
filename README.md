# cmpe297_assignment2

Folder Structure and their content

lm_studio.ipynb - contains code for how you can access llms running locally via lm studio via their APIs

### llama_factory

1. DPO_Finetuning.ipynb - fine tuning Qwen/Qwen2-VL-7B-Instruct using llama factory and DPO fine tuning (requires colab pro)

2. PPO_Finetuning.ipynb - fine tunning meta/llama3-8B using llama factory and DPO fine tuning (requires colab pro)

3. SupervisedFineTuning.ipynb - fine tuining unsloth/llama-3-8b-Instruct-bnb-4bit using llama factory and Supervised fine tuning technique (doesn't require colab pro)

### ollama

1. ollama_local.ipynb - has code and screenshots of how you can run various LLMs models locally using ollama, it also contains code for multimodal inference and how you can access the locally running LLMs through API. 

2. Running_Ollama_on_Colab.ipnb - has instructions on how you can run ollama on the free version of colab, as well as how you can use langchain to invoke the ollama models running on your colab notebook as APIs.



# Demo Video

1. LLamaFactory: https://youtu.be/6hJBy9Re0mo

2. LM studio: https://youtu.be/JV4Y8uMxo1k

3. Ollama: https://youtu.be/hPDswFmFWS4

4. Dify: https://youtu.be/dqzs2nDYtak